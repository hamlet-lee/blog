# Presto Planer 的改进
https://prestodb.io/blog/2019/12/23/improve-presto-planner  

总的来说，会支持更好的 ppd。  
有空可以学习下。  

学习记录：  
这个改进让Connector可以提供optimizer。optimizer 面对的是已经resolve好的节点。方便shared。  
optimizer的时机是在 add exchange之前，以及计划完成之前。  

依赖于这次改进，fb有个提高hive读取性能的扩展aria： 
https://engineering.fb.com/data-infrastructure/aria-presto/  
似乎可以提高性能不少。  

另外，似乎可以用于实现"行级别"的数据安全。即可通过一个SecurityConnector来包裹实际的Connector，从而可以插入Filter.

# 关于 Map 的性能
这篇提到一些改进
https://prestodb.io/blog/2019/09/26/tablescan-structs

在aria项目中，struct能够达到原生列的性能了；  
对于map，则仍然慢。  
Presto 的 ORC 分支 DWRF 对map存储做了优化（称为flat map），即每个stripe使用原生列存储 Map。使得 map 能达到 struct 并能达到 top level column 的性能。

# 关于 Presto Memory 管理
https://prestodb.io/blog/2019/08/19/memory-tracking
里面提到，内存分为 System 和 User 两种用途。  
User 指的是跟 Query 具体数据量，Group By 的基数相关的用途；  
System 指的是系统性的用途，例如 Buffer 等。  

内存块则有 General 和 Reserved。  
General 用于正常情况，Reserved 则用于部分 worker 的 general 告急时使用。Reserved 启时，会在全 Cluster 级别把 Reserved 内存分配给"最大"的那个 Query，让它尽早完成。-- 这种情况外部是否能检测出来？  

Reserved的量为 *query.max-total-memory-per-node* 设定的大小。  

用户可以选择关闭 Reserved 机制（这个机制太费内存），并同时设定启用 OOM Killer 机制，杀掉费内存导致 Cluster 不正常的 Query。  

当单个 Query 使用的内存超限时，会被杀。 错误代码：EXCEEDED_LOCAL_MEMORY_LIMIT，EXCEEDED_GLOBAL_MEMORY_LIMIT。  
限制参数  
* query.max-memory-per-node = worker level *user* memory limit  
* query.max-total-memory-per-node = worker level total (*user* + *system*) memory
  
query.max-memory and query.max-total-memory  相应限制 cluster level 的上限。  

* 似乎这里的限制的是单个 query 的内存消耗，之前我可能理解错了！！！  

* 使用 Memroy Context 来跟踪内存使用，Memory Context 组成树状结构，通过OperatorContext 提供给Operator。  Operator 用 setBytes（N） 来告知自己所用的内存量。  

* 提供 REST 接口向外暴露内存相关数据

* 使用 memroy.heap-headroom-per-node 来预留一些其它用途的内存。

* Coordinator 不仅会处理 limits，还会OOM Killer，以及 Detect Memory Leak - Query 已完成，但Reserved 不为0 
