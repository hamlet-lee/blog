# ANTLR4 Grammer
* 在线文档：https://github.com/antlr/antlr4/blob/master/doc/index.md

* 语法概述： https://github.com/antlr/antlr4/blob/master/doc/grammars.md

  >> 'Parser rule' names must start with a lowercase letter and 'lexer rules' must start with a capital letter.

* vscode 中可以使用插件 “ANTLR4 grammar syntax support”

* ANTLR4 语法片段解读
  ```g4
  statement
      : query                                                            #statementDefault
      | USE schema=identifier                                            #use
      | USE catalog=identifier '.' schema=identifier   
  ```
  1. `statement :` -> 这是 parser rule
  1. `shcema=identifier` -> 出现的rule `identifier` 变成 `schema` 成员
  1. `query` -> 首选 rule `query`

  ```g4
  singleStatement
    : statement EOF
    ;
  ```
  单 statement 定义， 这里 `EOF` 是什么呢？参考：https://stackoverflow.com/questions/17844248/when-is-eof-needed-in-antlr-4
  
  >> You should include an explicit EOF at the end of your entry rule any time you are trying to parse an entire input file. If you do not include the EOF, it means you are not trying to parse the entire input, and it's acceptable to parse only a portion of the input if it means avoiding a syntax error.

# Presto 源码分析 - Parse
以下是CLI发送 `select 1;` 服务端解析sql的调用栈
```text
"query-scheduler-856@41305" prio=5 tid=0xbc27 nid=NA runnable
  java.lang.Thread.State: RUNNABLE
      at com.facebook.presto.sql.parser.SqlParser.createStatement(SqlParser.java:98)
      at com.facebook.presto.execution.QueryPreparer.prepareQuery(QueryPreparer.java:56)
      at com.facebook.presto.execution.SqlQueryManager.createQueryInternal(SqlQueryManager.java:343)
      at com.facebook.presto.execution.SqlQueryManager.lambda$createQuery$4(SqlQueryManager.java:305)
      at com.facebook.presto.execution.SqlQueryManager$$Lambda$1193.1572680147.run(Unknown Source:-1)
      at com.facebook.presto.$gen.Presto_0_222_27c7628____20190805_074551_1.run(Unknown Source:-1)
      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      at java.lang.Thread.run(Thread.java:748)
```

SqlQueryManager 是总管性质的
```java
// prepare query
            preparedQuery = queryPreparer.prepareQuery(session, query, warningCollector);

            // select resource group
            queryType = getQueryType(preparedQuery.getStatement().getClass());
            selectionContext = resourceGroupManager.selectGroup(new SelectionCriteria(
                    sessionContext.getIdentity().getPrincipal().isPresent(),
                    sessionContext.getIdentity().getUser(),
                    Optional.ofNullable(sessionContext.getSource()),
                    sessionContext.getClientTags(),
                    sessionContext.getResourceEstimates(),
                    queryType.map(Enum::name)));

            // apply system defaults for query
            session = sessionPropertyDefaults.newSessionWithDefaultProperties(session, queryType.map(Enum::name), selectionContext.getResourceGroupId());

            // mark existing transaction as active
            transactionManager.activateTransaction(session, isTransactionControlStatement(preparedQuery.getStatement()), accessControl);

            // create query execution
            QueryExecutionFactory<?> queryExecutionFactory = executionFactories.get(preparedQuery.getStatement().getClass());
            if (queryExecutionFactory == null) {
                throw new PrestoException(NOT_SUPPORTED, "Unsupported statement type: " + preparedQuery.getStatement().getClass().getSimpleName());
            }
            queryExecution = queryExecutionFactory.createQueryExecution(
                    query,
                    session,
                    preparedQuery,
                    selectionContext.getResourceGroupId(),
                    warningCollector,
                    queryType);
            
            ...
            
            resourceGroupManager.submit(preparedQuery.getStatement(), queryExecution, selectionContext, queryExecutor);
```

QueryType 则是语句类型
```java
public enum QueryType
{
    DATA_DEFINITION,
    DELETE,
    DESCRIBE,
    EXPLAIN,
    ANALYZE,
    INSERT,
    SELECT
}
```

sessionContext 有很多客户端环境属性

resourceGroupManager.submit() 是提交执行

InternalResourceGroup 会接受queryExecution，并执行
```java
 public void run(ManagedQueryExecution query)
    {
        synchronized (root) {
            if (!subGroups.isEmpty()) {
                throw new PrestoException(INVALID_RESOURCE_GROUP, format("Cannot add queries to %s. It is not a leaf group.", id));
            }
            // Check all ancestors for capacity
            InternalResourceGroup group = this;
            boolean canQueue = true;
            boolean canRun = true;
            while (true) {
                canQueue &= group.canQueueMore();
                canRun &= group.canRunMore();
                if (!group.parent.isPresent()) {
                    break;
                }
                group = group.parent.get();
            }
            if (!canQueue && !canRun) {
                query.fail(new QueryQueueFullException(id));
                return;
            }
            if (canRun) {
            // 开跑
                startInBackground(query);
            }
            else {
            // 当前队列处于不能跑状态，入队
                enqueueQuery(query);
            }
            query.addStateChangeListener(state -> {
                if (state.isDone()) {
                    queryFinished(query);
                }
            });
        }
    }
```

```java
  private void startInBackground(ManagedQueryExecution query)
    {
        checkState(Thread.holdsLock(root), "Must hold lock to start a query");
        synchronized (root) {
            runningQueries.add(query);
            InternalResourceGroup group = this;
            while (group.parent.isPresent()) {
                group.parent.get().descendantRunningQueries++;
                group.parent.get().dirtySubGroups.add(group);
                group = group.parent.get();
            }
            updateEligibility();
        // 提交给executor跑，query::start是实际工作
            executor.execute(query::start);
        }
    }
```

通常的SELECT语句，则query的类型是SqlQueryExecution。
=这个设计非常松耦合，自己设计的Query类型也可以很容易地融入进来=

等到worker就开始工作, SqlQueryExecution.java
```java
    private void waitForMinimumWorkers()
    {
        ListenableFuture<?> minimumWorkerFuture = clusterSizeMonitor.waitForMinimumWorkers();
        addSuccessCallback(minimumWorkerFuture, () -> queryExecutor.submit(this::startExecution));
        addExceptionCallback(minimumWorkerFuture, throwable -> queryExecutor.submit(() -> stateMachine.transitionToFailed(throwable)));
    }
```

startExecution 是核心逻辑
```java
 private void startExecution()
    {
        try (SetThreadName ignored = new SetThreadName("Query-%s", stateMachine.getQueryId())) {
            try {
                // transition to planning
                if (!stateMachine.transitionToPlanning()) {
                    // query already started or finished
                    return;
                }

                // analyze query
                PlanRoot plan = analyzeQuery();

                metadata.beginQuery(getSession(), plan.getConnectors());

                // plan distribution of query
                planDistribution(plan);

                // transition to starting
                if (!stateMachine.transitionToStarting()) {
                    // query already started or finished
                    return;
                }

                // if query is not finished, start the scheduler, otherwise cancel it
                SqlQueryScheduler scheduler = queryScheduler.get();

                if (!stateMachine.isDone()) {
                    scheduler.start();
                }
            }
            catch (Throwable e) {
                fail(e);
                throwIfInstanceOf(e, Error.class);
            }
        }
    }
```

analyzeQuery -> doAnalyzeQuery  
后者做了optimization和fragment
```java
private PlanRoot doAnalyzeQuery()
    {
        // time analysis phase
        stateMachine.beginAnalysis();

        // plan query
        PlanNodeIdAllocator idAllocator = new PlanNodeIdAllocator();
        LogicalPlanner logicalPlanner = new LogicalPlanner(false, stateMachine.getSession(), planOptimizers, idAllocator, metadata, sqlParser, statsCalculator, costCalculator, stateMachine.getWarningCollector());
        Plan plan = logicalPlanner.plan(analysis);
        queryPlan.set(plan);

        // extract inputs
        List<Input> inputs = new InputExtractor(metadata, stateMachine.getSession()).extractInputs(plan.getRoot());
        stateMachine.setInputs(inputs);

        // extract output
        Optional<Output> output = new OutputExtractor().extractOutput(plan.getRoot());
        stateMachine.setOutput(output);

        // fragment the plan
        SubPlan fragmentedPlan = planFragmenter.createSubPlans(stateMachine.getSession(), plan, false, idAllocator, stateMachine.getWarningCollector());

        // record analysis time
        stateMachine.endAnalysis();

        boolean explainAnalyze = analysis.getStatement() instanceof Explain && ((Explain) analysis.getStatement()).isAnalyze();
        return new PlanRoot(fragmentedPlan, !explainAnalyze, extractConnectors(analysis));
    }
```


`SELECT 1;` explain 结果如下：
```text
presto:default> explain select 1;
                                      Query Plan                                       
---------------------------------------------------------------------------------------
 - Output[_col0] => [expr:integer]                                                     
         Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}             
         _col0 := expr                                                                 
     - Project[] => [expr:integer]                                                     
             Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}         
             expr := INTEGER 1                                                         
         - LocalExchange[ROUND_ROBIN] () => []                                         
                 Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00}     
             - Values => []                                                            
                     Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00} 
                     ()  
```
idea中查看plan对象，其内容是相符的  
![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-08-31/logical_plan.png)

CLI中显示Distributed Plan的内容，如下  

```text
presto:default> explain (TYPE DISTRIBUTED) select 1;
                                        Query Plan                                        
------------------------------------------------------------------------------------------
 Fragment 0 [SINGLE]                                                                      
     Output layout: [expr]                                                                
     Output partitioning: SINGLE []                                                       
     Stage Execution Strategy: UNGROUPED_EXECUTION                                        
     - Output[_col0] => [expr:integer]                                                    
             Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}            
             _col0 := expr                                                                
         - Project[] => [expr:integer]                                                    
                 Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}        
                 expr := INTEGER 1                                                        
             - LocalExchange[ROUND_ROBIN] () => []                                        
                     Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00}    
                 - Values => []                                                           
                         Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00}
                         ()                                                               
```
idea中查看fragmentedPlan对象，其内容相符:  
![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-08-31/fragmented_plan.png)

复杂一些的查询：
```text
presto:default> explain select count(*) from mydb.mytbl where day = '2019-08-01';
                                                                                                            Query Plan                                                             
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 - Output[_col0] => [count:bigint]                                                                                                                                                 
         _col0 := count                                                                                                                                                            
     - Aggregate(FINAL) => [count:bigint]                                                                                                                                          
             count := "count"((count_3))                                                                                                                                           
         - LocalExchange[SINGLE] () => [count_3:bigint]                                                                                                                            
             - RemoteStreamingExchange[GATHER] => [count_3:bigint]                                                                                                                 
                 - Aggregate(PARTIAL) => [count_3:bigint]                                                                                                                          
                         count_3 := "count"(*)                                                                                                                                     
                     - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValues=Optional.empty}'
                             Estimates: {rows: 11475 (0B), cpu: 0.00, memory: 0.00, network: 0.00}                                                                                 
                             LAYOUT: mydb.mytbl                                                                                                                             
                             day:string:-1:PARTITION_KEY                                                                                                                           
                                 :: [[2019-08-01]]                                                                                                                                 
                                                                                                                                                                                   
(1 row)

Query 20190831_115019_00016_e9k2i, FINISHED, 1 node
Splits: 1 total, 1 done (100.00%)
0:03 [0 rows, 0B] [0 rows/s, 0B/s]

presto:default> explain (TYPE DISTRIBUTED) select count(*) from mydb.mytbl where day = '2019-08-01';
                                                                                                              Query Plan                                                           
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Fragment 0 [SINGLE]                                                                                                                                                               
     Output layout: [count]                                                                                                                                                        
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Output[_col0] => [count:bigint]                                                                                                                                             
             _col0 := count                                                                                                                                                        
         - Aggregate(FINAL) => [count:bigint]                                                                                                                                      
                 count := "count"((count_3))                                                                                                                                       
             - LocalExchange[SINGLE] () => [count_3:bigint]                                                                                                                        
                 - RemoteSource[1] => [count_3:bigint]                                                                                                                             
                                                                                                                                                                                   
 Fragment 1 [SOURCE]                                                                                                                                                               
     Output layout: [count_3]                                                                                                                                                      
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Aggregate(PARTIAL) => [count_3:bigint]                                                                                                                                      
             count_3 := "count"(*)                                                                                                                                                 
         - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValues=Optional.empty}', layout='Op
                 Estimates: {rows: 11475 (0B), cpu: 0.00, memory: 0.00, network: 0.00}                                                                                             
                 LAYOUT: mydb.mytbl                                                                                                                                         
                 day:string:-1:PARTITION_KEY                                                                                                                                       
                     :: [[2019-08-01]]                                                                                                                                             
```

更复杂的查询:  
```text
presto:default> explain select count(*) from mydb.mytbl where day = '2019-08-01' group by day;
                                                                                                                                     Query Plan                                    
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 - Output[_col0] => [count:bigint]                                                                                                                                                 
         _col0 := count                                                                                                                                                            
     - RemoteStreamingExchange[GATHER] => [count:bigint]                                                                                                                           
         - Project[] => [count:bigint]                                                                                                                                             
             - Aggregate(FINAL)[day][$hashvalue] => [day:varchar, $hashvalue:bigint, count:bigint]                                                                                 
                     count := "count"((count_4))                                                                                                                                   
                 - LocalExchange[HASH][$hashvalue] ("day") => [day:varchar, count_4:bigint, $hashvalue:bigint]                                                                     
                     - RemoteStreamingExchange[REPARTITION][$hashvalue_5] => [day:varchar, count_4:bigint, $hashvalue_5:bigint]                                                    
                         - Aggregate(PARTIAL)[day][$hashvalue_6] => [day:varchar, $hashvalue_6:bigint, count_4:bigint]                                                             
                                 count_4 := "count"(*)                                                                                                                             
                             - ScanProject[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValue
                                     Estimates: {rows: 11475 (268.95kB), cpu: 172125.00, memory: 0.00, network: 0.00}/{rows: 11475 (268.95kB), cpu: 447525.00, memory: 0.00, networ
                                     $hashvalue_6 := combine_hash(CAST(VARCHAR 0 AS bigint), COALESCE($operator$hash_code(day), INTEGER 0))                                        
                                     LAYOUT: mydb.mytbl                                                                                                                     
                                     day := day:string:-1:PARTITION_KEY                                                                                                            
                                         :: [[2019-08-01]]



presto:default> explain (TYPE DISTRIBUTED) select count(*) from mydb.mytbl where day = '2019-08-01' group by day;
                                                                                                                                   Query Plan                                      
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Fragment 0 [SINGLE]                                                                                                                                                               
     Output layout: [count]                                                                                                                                                        
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Output[_col0] => [count:bigint]                                                                                                                                             
             _col0 := count                                                                                                                                                        
         - RemoteSource[1] => [count:bigint]                                                                                                                                       
                                                                                                                                                                                   
 Fragment 1 [HASH]                                                                                                                                                                 
     Output layout: [count]                                                                                                                                                        
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Project[] => [count:bigint]                                                                                                                                                 
         - Aggregate(FINAL)[day][$hashvalue] => [day:varchar, $hashvalue:bigint, count:bigint]                                                                                     
                 count := "count"((count_4))                                                                                                                                       
             - LocalExchange[HASH][$hashvalue] ("day") => [day:varchar, count_4:bigint, $hashvalue:bigint]                                                                         
                 - RemoteSource[2] => [day:varchar, count_4:bigint, $hashvalue_5:bigint]                                                                                           
                                                                                                                                                                                   
 Fragment 2 [SOURCE]                                                                                                                                                               
     Output layout: [day, count_4, $hashvalue_6]                                                                                                                                   
     Output partitioning: HASH [day][$hashvalue_6]                                                                                                                                 
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Aggregate(PARTIAL)[day][$hashvalue_6] => [day:varchar, $hashvalue_6:bigint, count_4:bigint]                                                                                 
             count_4 := "count"(*)                                                                                                                                                 
         - ScanProject[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValues=Optional.empty}', 
                 Estimates: {rows: 11475 (268.95kB), cpu: 172125.00, memory: 0.00, network: 0.00}/{rows: 11475 (268.95kB), cpu: 447525.00, memory: 0.00, network: 0.00}            
                 $hashvalue_6 := combine_hash(CAST(VARCHAR 0 AS bigint), COALESCE($operator$hash_code(day), INTEGER 0))                                                            
                 LAYOUT: mydb.mytbl                                                                                                                                         
                 day := day:string:-1:PARTITION_KEY                                                                                                                                
                     :: [[2019-08-01]]                                                                                                                                             

```

关注Exchange，可以发现
查询1（`select 1`）:
```text
LocalExchange[ROUND_ROBIN]

->
  Fragment 0
    LocalExchange[ROUND_ROBIN]
```

查询2 （`select count(*) from mydb.mytbl where day = '2019-08-01'`）:
```text
LocalExchange[SINGLE]
  RemoteStreamingExchange[GATHER]

->
  Fragment 0
    LocalExchange[SINGLE] () => [count_3:bigint]                                                                         RemoteSource[1]                                               
  Fragment 1
    Output partitioning: SINGLE [] 
```
查询3 (`select count(*) from mydb.mytbl where day = '2019-08-01' group by day`):
```text
RemoteStreamingExchange[GATHER]
  LocalExchange[HASH][$hashvalue]
    RemoteStreamingExchange[REPARTITION]
->
  Fragment 0
    RemoteSource[1]
  Fragment 1
    LocalExchange[HASH][$hashvalue]
      RemoteSource[2]
  Fragment 2
    Output partitioning: HASH [day][$hashvalue_6]
```

可见，确实是 RemoteStreamingExchange 会导致 Fragment拆分。