# ANTLR4 Grammer
* 在线文档：https://github.com/antlr/antlr4/blob/master/doc/index.md

* 语法概述： https://github.com/antlr/antlr4/blob/master/doc/grammars.md

  >> 'Parser rule' names must start with a lowercase letter and 'lexer rules' must start with a capital letter.

* vscode 中可以使用插件 “ANTLR4 grammar syntax support”

* ANTLR4 语法片段解读
  ```g4
  statement
      : query                                                            #statementDefault
      | USE schema=identifier                                            #use
      | USE catalog=identifier '.' schema=identifier   
  ```
  1. `statement :` -> 这是 parser rule
  1. `shcema=identifier` -> 出现的rule `identifier` 变成 `schema` 成员
  1. `query` -> 首选 rule `query`

  ```g4
  singleStatement
    : statement EOF
    ;
  ```
  单 statement 定义， 这里 `EOF` 是什么呢？参考：https://stackoverflow.com/questions/17844248/when-is-eof-needed-in-antlr-4
  
  >> You should include an explicit EOF at the end of your entry rule any time you are trying to parse an entire input file. If you do not include the EOF, it means you are not trying to parse the entire input, and it's acceptable to parse only a portion of the input if it means avoiding a syntax error.

# Presto 源码分析 - Parse & Plan
以下是CLI发送 `select 1;` 服务端解析sql的调用栈
```text
"query-scheduler-856@41305" prio=5 tid=0xbc27 nid=NA runnable
  java.lang.Thread.State: RUNNABLE
      at com.facebook.presto.sql.parser.SqlParser.createStatement(SqlParser.java:98)
      at com.facebook.presto.execution.QueryPreparer.prepareQuery(QueryPreparer.java:56)
      at com.facebook.presto.execution.SqlQueryManager.createQueryInternal(SqlQueryManager.java:343)
      at com.facebook.presto.execution.SqlQueryManager.lambda$createQuery$4(SqlQueryManager.java:305)
      at com.facebook.presto.execution.SqlQueryManager$$Lambda$1193.1572680147.run(Unknown Source:-1)
      at com.facebook.presto.$gen.Presto_0_222_27c7628____20190805_074551_1.run(Unknown Source:-1)
      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      at java.lang.Thread.run(Thread.java:748)
```

SqlQueryManager 是总管性质的
```java
// prepare query
            preparedQuery = queryPreparer.prepareQuery(session, query, warningCollector);

            // select resource group
            queryType = getQueryType(preparedQuery.getStatement().getClass());
            selectionContext = resourceGroupManager.selectGroup(new SelectionCriteria(
                    sessionContext.getIdentity().getPrincipal().isPresent(),
                    sessionContext.getIdentity().getUser(),
                    Optional.ofNullable(sessionContext.getSource()),
                    sessionContext.getClientTags(),
                    sessionContext.getResourceEstimates(),
                    queryType.map(Enum::name)));

            // apply system defaults for query
            session = sessionPropertyDefaults.newSessionWithDefaultProperties(session, queryType.map(Enum::name), selectionContext.getResourceGroupId());

            // mark existing transaction as active
            transactionManager.activateTransaction(session, isTransactionControlStatement(preparedQuery.getStatement()), accessControl);

            // create query execution
            QueryExecutionFactory<?> queryExecutionFactory = executionFactories.get(preparedQuery.getStatement().getClass());
            if (queryExecutionFactory == null) {
                throw new PrestoException(NOT_SUPPORTED, "Unsupported statement type: " + preparedQuery.getStatement().getClass().getSimpleName());
            }
            queryExecution = queryExecutionFactory.createQueryExecution(
                    query,
                    session,
                    preparedQuery,
                    selectionContext.getResourceGroupId(),
                    warningCollector,
                    queryType);
            
            ...
            
            resourceGroupManager.submit(preparedQuery.getStatement(), queryExecution, selectionContext, queryExecutor);
```

QueryType 则是语句类型
```java
public enum QueryType
{
    DATA_DEFINITION,
    DELETE,
    DESCRIBE,
    EXPLAIN,
    ANALYZE,
    INSERT,
    SELECT
}
```

sessionContext 有很多客户端环境属性

resourceGroupManager.submit() 是提交执行

InternalResourceGroup 会接受queryExecution，并执行
```java
 public void run(ManagedQueryExecution query)
    {
        synchronized (root) {
            if (!subGroups.isEmpty()) {
                throw new PrestoException(INVALID_RESOURCE_GROUP, format("Cannot add queries to %s. It is not a leaf group.", id));
            }
            // Check all ancestors for capacity
            InternalResourceGroup group = this;
            boolean canQueue = true;
            boolean canRun = true;
            while (true) {
                canQueue &= group.canQueueMore();
                canRun &= group.canRunMore();
                if (!group.parent.isPresent()) {
                    break;
                }
                group = group.parent.get();
            }
            if (!canQueue && !canRun) {
                query.fail(new QueryQueueFullException(id));
                return;
            }
            if (canRun) {
            // 开跑
                startInBackground(query);
            }
            else {
            // 当前队列处于不能跑状态，入队
                enqueueQuery(query);
            }
            query.addStateChangeListener(state -> {
                if (state.isDone()) {
                    queryFinished(query);
                }
            });
        }
    }
```

```java
  private void startInBackground(ManagedQueryExecution query)
    {
        checkState(Thread.holdsLock(root), "Must hold lock to start a query");
        synchronized (root) {
            runningQueries.add(query);
            InternalResourceGroup group = this;
            while (group.parent.isPresent()) {
                group.parent.get().descendantRunningQueries++;
                group.parent.get().dirtySubGroups.add(group);
                group = group.parent.get();
            }
            updateEligibility();
        // 提交给executor跑，query::start是实际工作
            executor.execute(query::start);
        }
    }
```

通常的SELECT语句，则query的类型是SqlQueryExecution。
=这个设计非常松耦合，自己设计的Query类型也可以很容易地融入进来=

等到worker就开始工作, SqlQueryExecution.java
```java
    private void waitForMinimumWorkers()
    {
        ListenableFuture<?> minimumWorkerFuture = clusterSizeMonitor.waitForMinimumWorkers();
        addSuccessCallback(minimumWorkerFuture, () -> queryExecutor.submit(this::startExecution));
        addExceptionCallback(minimumWorkerFuture, throwable -> queryExecutor.submit(() -> stateMachine.transitionToFailed(throwable)));
    }
```

startExecution 是核心逻辑
```java
 private void startExecution()
    {
        try (SetThreadName ignored = new SetThreadName("Query-%s", stateMachine.getQueryId())) {
            try {
                // transition to planning
                if (!stateMachine.transitionToPlanning()) {
                    // query already started or finished
                    return;
                }

                // analyze query
                PlanRoot plan = analyzeQuery();

                metadata.beginQuery(getSession(), plan.getConnectors());

                // plan distribution of query
                planDistribution(plan);

                // transition to starting
                if (!stateMachine.transitionToStarting()) {
                    // query already started or finished
                    return;
                }

                // if query is not finished, start the scheduler, otherwise cancel it
                SqlQueryScheduler scheduler = queryScheduler.get();

                if (!stateMachine.isDone()) {
                    scheduler.start();
                }
            }
            catch (Throwable e) {
                fail(e);
                throwIfInstanceOf(e, Error.class);
            }
        }
    }
```

analyzeQuery -> doAnalyzeQuery  
后者做了optimization和fragment
```java
private PlanRoot doAnalyzeQuery()
    {
        // time analysis phase
        stateMachine.beginAnalysis();

        // plan query
        PlanNodeIdAllocator idAllocator = new PlanNodeIdAllocator();
        LogicalPlanner logicalPlanner = new LogicalPlanner(false, stateMachine.getSession(), planOptimizers, idAllocator, metadata, sqlParser, statsCalculator, costCalculator, stateMachine.getWarningCollector());
        Plan plan = logicalPlanner.plan(analysis);
        queryPlan.set(plan);

        // extract inputs
        List<Input> inputs = new InputExtractor(metadata, stateMachine.getSession()).extractInputs(plan.getRoot());
        stateMachine.setInputs(inputs);

        // extract output
        Optional<Output> output = new OutputExtractor().extractOutput(plan.getRoot());
        stateMachine.setOutput(output);

        // fragment the plan
        SubPlan fragmentedPlan = planFragmenter.createSubPlans(stateMachine.getSession(), plan, false, idAllocator, stateMachine.getWarningCollector());

        // record analysis time
        stateMachine.endAnalysis();

        boolean explainAnalyze = analysis.getStatement() instanceof Explain && ((Explain) analysis.getStatement()).isAnalyze();
        return new PlanRoot(fragmentedPlan, !explainAnalyze, extractConnectors(analysis));
    }
```


`SELECT 1;` explain 结果如下：
```text
presto:default> explain select 1;
                                      Query Plan                                       
---------------------------------------------------------------------------------------
 - Output[_col0] => [expr:integer]                                                     
         Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}             
         _col0 := expr                                                                 
     - Project[] => [expr:integer]                                                     
             Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}         
             expr := INTEGER 1                                                         
         - LocalExchange[ROUND_ROBIN] () => []                                         
                 Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00}     
             - Values => []                                                            
                     Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00} 
                     ()  
```
idea中查看plan对象，其内容是相符的  
![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-08-31/logical_plan.png)

CLI中显示Distributed Plan的内容，如下  

```text
presto:default> explain (TYPE DISTRIBUTED) select 1;
                                        Query Plan                                        
------------------------------------------------------------------------------------------
 Fragment 0 [SINGLE]                                                                      
     Output layout: [expr]                                                                
     Output partitioning: SINGLE []                                                       
     Stage Execution Strategy: UNGROUPED_EXECUTION                                        
     - Output[_col0] => [expr:integer]                                                    
             Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}            
             _col0 := expr                                                                
         - Project[] => [expr:integer]                                                    
                 Estimates: {rows: 1 (5B), cpu: 5.00, memory: 0.00, network: 0.00}        
                 expr := INTEGER 1                                                        
             - LocalExchange[ROUND_ROBIN] () => []                                        
                     Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00}    
                 - Values => []                                                           
                         Estimates: {rows: 1 (0B), cpu: 0.00, memory: 0.00, network: 0.00}
                         ()                                                               
```
idea中查看fragmentedPlan对象，其内容相符:  
![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-08-31/fragmented_plan.png)

复杂一些的查询：
```text
presto:default> explain select count(*) from mydb.mytbl where day = '2019-08-01';
                                                                                                            Query Plan                                                             
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 - Output[_col0] => [count:bigint]                                                                                                                                                 
         _col0 := count                                                                                                                                                            
     - Aggregate(FINAL) => [count:bigint]                                                                                                                                          
             count := "count"((count_3))                                                                                                                                           
         - LocalExchange[SINGLE] () => [count_3:bigint]                                                                                                                            
             - RemoteStreamingExchange[GATHER] => [count_3:bigint]                                                                                                                 
                 - Aggregate(PARTIAL) => [count_3:bigint]                                                                                                                          
                         count_3 := "count"(*)                                                                                                                                     
                     - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValues=Optional.empty}'
                             Estimates: {rows: 11475 (0B), cpu: 0.00, memory: 0.00, network: 0.00}                                                                                 
                             LAYOUT: mydb.mytbl                                                                                                                             
                             day:string:-1:PARTITION_KEY                                                                                                                           
                                 :: [[2019-08-01]]                                                                                                                                 
                                                                                                                                                                                   
(1 row)

Query 20190831_115019_00016_e9k2i, FINISHED, 1 node
Splits: 1 total, 1 done (100.00%)
0:03 [0 rows, 0B] [0 rows/s, 0B/s]

presto:default> explain (TYPE DISTRIBUTED) select count(*) from mydb.mytbl where day = '2019-08-01';
                                                                                                              Query Plan                                                           
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Fragment 0 [SINGLE]                                                                                                                                                               
     Output layout: [count]                                                                                                                                                        
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Output[_col0] => [count:bigint]                                                                                                                                             
             _col0 := count                                                                                                                                                        
         - Aggregate(FINAL) => [count:bigint]                                                                                                                                      
                 count := "count"((count_3))                                                                                                                                       
             - LocalExchange[SINGLE] () => [count_3:bigint]                                                                                                                        
                 - RemoteSource[1] => [count_3:bigint]                                                                                                                             
                                                                                                                                                                                   
 Fragment 1 [SOURCE]                                                                                                                                                               
     Output layout: [count_3]                                                                                                                                                      
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Aggregate(PARTIAL) => [count_3:bigint]                                                                                                                                      
             count_3 := "count"(*)                                                                                                                                                 
         - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValues=Optional.empty}', layout='Op
                 Estimates: {rows: 11475 (0B), cpu: 0.00, memory: 0.00, network: 0.00}                                                                                             
                 LAYOUT: mydb.mytbl                                                                                                                                         
                 day:string:-1:PARTITION_KEY                                                                                                                                       
                     :: [[2019-08-01]]                                                                                                                                             
```

更复杂的查询:  
```text
presto:default> explain select count(*) from mydb.mytbl where day = '2019-08-01' group by day;
                                                                                                                                     Query Plan                                    
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 - Output[_col0] => [count:bigint]                                                                                                                                                 
         _col0 := count                                                                                                                                                            
     - RemoteStreamingExchange[GATHER] => [count:bigint]                                                                                                                           
         - Project[] => [count:bigint]                                                                                                                                             
             - Aggregate(FINAL)[day][$hashvalue] => [day:varchar, $hashvalue:bigint, count:bigint]                                                                                 
                     count := "count"((count_4))                                                                                                                                   
                 - LocalExchange[HASH][$hashvalue] ("day") => [day:varchar, count_4:bigint, $hashvalue:bigint]                                                                     
                     - RemoteStreamingExchange[REPARTITION][$hashvalue_5] => [day:varchar, count_4:bigint, $hashvalue_5:bigint]                                                    
                         - Aggregate(PARTIAL)[day][$hashvalue_6] => [day:varchar, $hashvalue_6:bigint, count_4:bigint]                                                             
                                 count_4 := "count"(*)                                                                                                                             
                             - ScanProject[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValue
                                     Estimates: {rows: 11475 (268.95kB), cpu: 172125.00, memory: 0.00, network: 0.00}/{rows: 11475 (268.95kB), cpu: 447525.00, memory: 0.00, networ
                                     $hashvalue_6 := combine_hash(CAST(VARCHAR 0 AS bigint), COALESCE($operator$hash_code(day), INTEGER 0))                                        
                                     LAYOUT: mydb.mytbl                                                                                                                     
                                     day := day:string:-1:PARTITION_KEY                                                                                                            
                                         :: [[2019-08-01]]



presto:default> explain (TYPE DISTRIBUTED) select count(*) from mydb.mytbl where day = '2019-08-01' group by day;
                                                                                                                                   Query Plan                                      
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Fragment 0 [SINGLE]                                                                                                                                                               
     Output layout: [count]                                                                                                                                                        
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Output[_col0] => [count:bigint]                                                                                                                                             
             _col0 := count                                                                                                                                                        
         - RemoteSource[1] => [count:bigint]                                                                                                                                       
                                                                                                                                                                                   
 Fragment 1 [HASH]                                                                                                                                                                 
     Output layout: [count]                                                                                                                                                        
     Output partitioning: SINGLE []                                                                                                                                                
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Project[] => [count:bigint]                                                                                                                                                 
         - Aggregate(FINAL)[day][$hashvalue] => [day:varchar, $hashvalue:bigint, count:bigint]                                                                                     
                 count := "count"((count_4))                                                                                                                                       
             - LocalExchange[HASH][$hashvalue] ("day") => [day:varchar, count_4:bigint, $hashvalue:bigint]                                                                         
                 - RemoteSource[2] => [day:varchar, count_4:bigint, $hashvalue_5:bigint]                                                                                           
                                                                                                                                                                                   
 Fragment 2 [SOURCE]                                                                                                                                                               
     Output layout: [day, count_4, $hashvalue_6]                                                                                                                                   
     Output partitioning: HASH [day][$hashvalue_6]                                                                                                                                 
     Stage Execution Strategy: UNGROUPED_EXECUTION                                                                                                                                 
     - Aggregate(PARTIAL)[day][$hashvalue_6] => [day:varchar, $hashvalue_6:bigint, count_4:bigint]                                                                                 
             count_4 := "count"(*)                                                                                                                                                 
         - ScanProject[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValues=Optional.empty}', 
                 Estimates: {rows: 11475 (268.95kB), cpu: 172125.00, memory: 0.00, network: 0.00}/{rows: 11475 (268.95kB), cpu: 447525.00, memory: 0.00, network: 0.00}            
                 $hashvalue_6 := combine_hash(CAST(VARCHAR 0 AS bigint), COALESCE($operator$hash_code(day), INTEGER 0))                                                            
                 LAYOUT: mydb.mytbl                                                                                                                                         
                 day := day:string:-1:PARTITION_KEY                                                                                                                                
                     :: [[2019-08-01]]                                                                                                                                             

```

关注Exchange，可以发现
查询1（`select 1`）:
```text
LocalExchange[ROUND_ROBIN]

->
  Fragment 0
    LocalExchange[ROUND_ROBIN]
```

查询2 （`select count(*) from mydb.mytbl where day = '2019-08-01'`）:
```text
LocalExchange[SINGLE]
  RemoteStreamingExchange[GATHER]

->
  Fragment 0
    LocalExchange[SINGLE] () => [count_3:bigint]                                                                         RemoteSource[1]                                               
  Fragment 1
    Output partitioning: SINGLE [] 
```
查询3 (`select count(*) from mydb.mytbl where day = '2019-08-01' group by day`):
```text
RemoteStreamingExchange[GATHER]
  LocalExchange[HASH][$hashvalue]
    RemoteStreamingExchange[REPARTITION]
->
  Fragment 0
    RemoteSource[1]
  Fragment 1
    LocalExchange[HASH][$hashvalue]
      RemoteSource[2]
  Fragment 2
    Output partitioning: HASH [day][$hashvalue_6]
```

可见，确实是 RemoteStreamingExchange 会导致 Fragment拆分。

查找 RemoteStreamingExchange，找到 PlanFragmenter.java
```java
 @Override
        public PlanNode visitExchange(ExchangeNode exchange, RewriteContext<FragmentProperties> context)
        {
            switch (exchange.getScope()) {
                case LOCAL:
                    return context.defaultRewrite(exchange, context.get());
                case REMOTE_STREAMING:
                    return createRemoteStreamingExchange(exchange, context);
                case REMOTE_MATERIALIZED:
                    return createRemoteMaterializedExchange(exchange, context);
                default:
                    throw new IllegalArgumentException("Unexpected exchange scope: " + exchange.getScope());
            }
        }
```

createRemoteStreamingExchange内容如下
```java
 private PlanNode createRemoteStreamingExchange(ExchangeNode exchange, RewriteContext<FragmentProperties> context)
        {
            checkArgument(exchange.getScope() == REMOTE_STREAMING, "Unexpected exchange scope: %s", exchange.getScope());

            PartitioningScheme partitioningScheme = exchange.getPartitioningScheme();

            if (exchange.getType() == ExchangeNode.Type.GATHER) {
                context.get().setSingleNodeDistribution();
            }
            else if (exchange.getType() == ExchangeNode.Type.REPARTITION) {
                context.get().setDistribution(partitioningScheme.getPartitioning().getHandle(), metadata, session);
            }

            ImmutableList.Builder<SubPlan> builder = ImmutableList.builder();
            for (int sourceIndex = 0; sourceIndex < exchange.getSources().size(); sourceIndex++) {
                FragmentProperties childProperties = new FragmentProperties(partitioningScheme.translateOutputLayout(exchange.getInputs().get(sourceIndex)));
                builder.add(buildSubPlan(exchange.getSources().get(sourceIndex), childProperties, context));
            }

            List<SubPlan> children = builder.build();
            context.get().addChildren(children);

            List<PlanFragmentId> childrenIds = children.stream()
                    .map(SubPlan::getFragment)
                    .map(PlanFragment::getId)
                    .collect(toImmutableList());

            return new RemoteSourceNode(exchange.getId(), childrenIds, exchange.getOutputVariables(), exchange.getOrderingScheme(), exchange.getType()); 
        }
```

貌似只有GATHER和REPARTITON，其它呢？

可以通过强制生成一个 `RemoteStreamingExchange[REPARTITION]` 达到并发的效果？

下面:
1. 看看udf节点的执行计划
1. 通过跟踪某个optimization的过程，熟悉plan改造的代码逻辑：调整和rewrite?
  * 思路：对比某个optimization执行前和执行后的状态来确定rewrite的效用
    * 具体方法可以试试参考 unit test，并在调试器中调用dump plan的方法

从比较简单的LimitPushDown.java分析：
笔记如下：  
![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-08-31/presto_optimization.jpg)  
可见，开发Optimizer的核心逻辑如下：
  1. 准备一个Rewriter，里面用visitXXX处理各种相关节点
  1. 准备一个Context对象，该对象在visitXXX可以共享
  1. PlanOptimizer在工作时，还可以引用一些环境信息以及工具性对象
  ```java
  public interface PlanOptimizer
{
    PlanNode optimize(PlanNode plan,
            Session session,
            TypeProvider types,
            SymbolAllocator symbolAllocator,
            PlanNodeIdAllocator idAllocator,
            WarningCollector warningCollector);
}

  ```

LimitPushDownRewriter的大致逻辑分析
![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-08-31/presto_limit_pushdown.jpg)  

还可以看看相关的DistinctLimitNode引入的Pull Request https://github.com/prestodb/presto/pull/906/files (在github上搜索DictinctLimitNode，在issues中即可找到)

在pull request中可以发现

presto-main/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java

这个文件里面，有从DistinctLimitNode对应到DistinctLimitOperatorFactory的逻辑：
```java
@Override
        public PhysicalOperation visitDistinctLimit(DistinctLimitNode node, LocalExecutionPlanContext context)
        {
            PhysicalOperation source = node.getSource().accept(this, context);
            OperatorFactory operatorFactory = new DistinctLimitOperatorFactory(
                    context.getNextOperatorId(),
                    source.getTupleInfos(),
                    node.getLimit());
            return new PhysicalOperation(operatorFactory, source.getLayout(), source);
        }
```

DistinctLimitOperatorFactory 生产了 DistinctLimitOperator。后者是真正的逻辑。

查看UDF的使用：
1. 一个参数的 `lower (string)`
查询 `select lower(my_userid) as mycol from mydb.mytbl where day = '2019-08-20'`
的plan内容
![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-08-31/lower_plan.png)

explain内容：
```text
presto:default> explain select lower(my_userid) as mycol from mydb.mytbl where day = '2019-08-20';
                                                                                                                 Query Plan                                                        
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 - Output[mycol] => [lower:varchar]                                                                                                                                                
         Estimates: {rows: 3559 (191.16kB), cpu: 391490.00, memory: 0.00, network: 195745.00}                                                                                      
         mycol := lower                                                                                                                                                            
     - RemoteStreamingExchange[GATHER] => [lower:varchar]                                                                                                                          
             Estimates: {rows: 3559 (191.16kB), cpu: 391490.00, memory: 0.00, network: 195745.00}                                                                                  
         - ScanProject[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=mydb, tableName=mytbl, analyzePartitionValues=Optional.empty}', 
                 Estimates: {rows: 3559 (191.16kB), cpu: 195745.00, memory: 0.00, network: 0.00}/{rows: 3559 (191.16kB), cpu: 391490.00, memory: 0.00, network: 0.00}              
                 lower := lower(my_userid)                                                                                                                                             
                 LAYOUT: mydb.mytbl                                                                                                                                         
                 my_userid := my_userid:string:25:REGULAR                                                                                                                                  
                 day:string:-1:PARTITION_KEY                                                                                                                                       
                     :: [[2019-08-20]]
```

可见，好像ProjectScanNode + TableScanNode 被合并成 ScanProject 了????  
为何内存里是这样？  
还有没有遗漏的处理？
