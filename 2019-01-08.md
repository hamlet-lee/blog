# 尽量保证Kafka Producer数据不丢失

* 参考1： https://blog.csdn.net/john2522/article/details/64555065

* 参考2： https://blog.csdn.net/Lin_wj1995/article/details/80264599

```
如果对性能要求不高的话，可以再 producer.send() 方法调用后再调用 producer.flush() 方法，该方法会将数据全部生产到Kafka，否则就会阻塞。对于 producer.flush() 方法，源码原话如下：

"Flush any accumulated records form the producer. Blocks until all sends are complete."
```

我觉得，如果允许一定量丢失，但不允许大量丢失的话，有一个折衷办法：

每10000个send，flush一次

# Logstash 如果出错，可以把出错数据单独输出

参考： https://discuss.elastic.co/t/exception-handling/38989

```
filter {
  date {
    ...
  }
}

output {
  if "_dateparsefailure" in [tags] {
    file {
      path => ".../date_parse_errors.log"
    }
  }
}
```

# Logstash 处理多种日期格式

参考：https://discuss.elastic.co/t/logstash-how-to-handle-exceptions/37843

```
match => ["DATE", "dd/MM/YYYY HH.mm.ss Z", "dd/MM/YYYY HH.mm.ss Z", "ISO8601"]
```

# Linux 管道输出到文件时不做buffer

* 参考： https://unix.stackexchange.com/questions/25372/turn-off-buffering-in-pipe

场景：在大文件big_file里面搜索一个很少见的字符串xxx，可能早已有输出，但是却由于输出数据较少，buffer一直不满，迟迟不输出到a.txt。
例如：

```shell
grep xxx big_file > a.txt
```

用unbuffer命令即可让输出立即到文件中
```
unbuffer grep xxx big_file > a.txt
```

# 设定Gradle的Home目录

```
export GRADLE_USER_HOME=/Users/mrhaki/dev/gradle
```

# Logstash 如果报错，注意把tags显示出来，看清楚是哪一步出错！

```
[2019-01-08T18:46:47,974][WARN ][logstash.codecs.plain    ] Received an event that has a different character encoding than you configured. {:text=>"119.147.183.23 - - [07/Jan/2019:00:00:02 +0800] \\\"GET /apps/log.html?xxxx=yyy&type=enter&q=Ê=ç± HTTP/1.1\\\" ...", :expected_charset=>"UTF-8"}
[2019-01-08T18:46:48,005][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9601}
{
       ...
      "message" => "...",
         "tags" => [
        [0] "_grokparsefailure"
    ],
           "ts" => 0
}

```

`_grokparsefailure` 表明是grok失败，不是kv或者别的失败！


# Logstash input codec 问题
对于accesslog，如果有二进制垃圾信息，会导致整行不能parse
可以尝试这样解决：

```
input {
  file {
    codec => plain {
      charset => "US-ASCII"
    }
}
```

虽然仍然会是一堆垃圾，但不会造成整行无法读入处理

# Logstash kv 插件可能生成数组

```
For example, consider a source like from=me from=me. [from] will map to an Array with two elements: ["me", "me"]. 
```

如果希望去重
```
filter {
  kv {
    allow_duplicate_values => false
  }
}
```

# Logstash 排除一些输入文件
```
file {
  path => "E:/domain/DEV/application/logs/.log"
  exclude => [ "CBA*.log", "BPL*.log" , "ARC*.log" ]
  type => "esbbwlog"
  codec => multiline {
    patterns_dir => "/patterns/devbw"
    pattern => "^%{BWTIME}"
    negate => true
    what => "previous"
    max_lines => "1000"
  }
}
```

# Logstash 监控进度

```shell
curl "http://localhost:9600/_node/stats?pretty"
```

返回类似

```json
{
  "host" : "xxx",
  "version" : "5.6.0",
  "http_address" : "127.0.0.1:9600",
  "id" : "552e763d-7d15-4247-b7c1-16e17d65fd29",
  "name" : "xxx",
  "jvm" : {
    "threads" : {
      "count" : 36,
      "peak_count" : 38
    },
    "mem" : {
      "heap_used_percent" : 31,
      "heap_committed_in_bytes" : 1037959168,
      "heap_max_in_bytes" : 1037959168,
      "heap_used_in_bytes" : 329612512,
      "non_heap_used_in_bytes" : 99764880,
      "non_heap_committed_in_bytes" : 105611264,
      "pools" : {
        "survivor" : {
          "peak_used_in_bytes" : 35782656,
          "used_in_bytes" : 6156528,
          "peak_max_in_bytes" : 35782656,
          "max_in_bytes" : 35782656,
          "committed_in_bytes" : 35782656
        },
        "old" : {
          "peak_used_in_bytes" : 127482200,
          "used_in_bytes" : 127482200,
          "peak_max_in_bytes" : 715849728,
          "max_in_bytes" : 715849728,
          "committed_in_bytes" : 715849728
        },
        "young" : {
          "peak_used_in_bytes" : 286326784,
          "used_in_bytes" : 195973784,
          "peak_max_in_bytes" : 286326784,
          "max_in_bytes" : 286326784,
          "committed_in_bytes" : 286326784
        }
      }
    },
    "gc" : {
      "collectors" : {
        "old" : {
          "collection_time_in_millis" : 175,
          "collection_count" : 1
        },
        "young" : {
          "collection_time_in_millis" : 27089,
          "collection_count" : 1991
        }
      }
    },
    "uptime_in_millis" : 619012
  },
  "process" : {
    "open_file_descriptors" : 63,
    "peak_open_file_descriptors" : 65,
    "max_file_descriptors" : 1048576,
    "mem" : {
      "total_virtual_in_bytes" : 3027443712
    },
    "cpu" : {
      "total_in_millis" : 3261290,
      "percent" : 68,
      "load_average" : {
        "1m" : 9.96,
        "5m" : 9.67,
        "15m" : 6.39
      }
    }
  },
  "pipeline" : {
    "events" : {
      "duration_in_millis" : 4245651,
      "in" : 3671839,
      "out" : 3671207,
      "filtered" : 3671207,
      "queue_push_duration_in_millis" : 414425
    },
    "plugins" : {
      "inputs" : [ {
        "id" : "26a2e0a2016f6ea2c037ee79aa1db183c1e02f36-2",
        "events" : {
          "out" : 3671839,
          "queue_push_duration_in_millis" : 414425
        },
        "name" : "file"
      } ],
      "filters" : [ ... ],
      "outputs" : [ {
        "id" : "26a2e0a2016f6ea2c037ee79aa1db183c1e02f36-14",
        "events" : {
          "duration_in_millis" : 331195,
          "in" : 3671207,
          "out" : 3671207
        },
        "name" : "pipe"
      } ]
    },
    "reloads" : {
      "last_error" : null,
      "successes" : 0,
      "last_success_timestamp" : null,
      "last_failure_timestamp" : null,
      "failures" : 0
    },
    "queue" : {
      "type" : "memory"
    },
    "id" : "main"
  },
  "reloads" : {
    "successes" : 0,
    "failures" : 0
  },
  "os" : { }
}
```

其中

```json
 "outputs" : [ {
        "id" : "26a2e0a2016f6ea2c037ee79aa1db183c1e02f36-14",
        "events" : {
          "duration_in_millis" : 331195,
          "in" : 3671207,
          "out" : 3671207
        },
        "name" : "pipe"
      } ]
```

就能看出处理速度
