# 尽量保证Kafka Producer数据不丢失

* 参考1： https://blog.csdn.net/john2522/article/details/64555065

* 参考2： https://blog.csdn.net/Lin_wj1995/article/details/80264599

```
如果对性能要求不高的话，可以再 producer.send() 方法调用后再调用 producer.flush() 方法，该方法会将数据全部生产到Kafka，否则就会阻塞。对于 producer.flush() 方法，源码原话如下：

"Flush any accumulated records form the producer. Blocks until all sends are complete."
```

我觉得，如果允许一定量丢失，但不允许大量丢失的话，有一个折衷办法：

每10000个send，flush一次

# Logstash 如果出错，可以把出错数据单独输出

参考： https://discuss.elastic.co/t/exception-handling/38989

```
filter {
  date {
    ...
  }
}

output {
  if "_dateparsefailure" in [tags] {
    file {
      path => ".../date_parse_errors.log"
    }
  }
}
```

# Logstash 处理多种日期格式

参考：https://discuss.elastic.co/t/logstash-how-to-handle-exceptions/37843

```
match => ["DATE", "dd/MM/YYYY HH.mm.ss Z", "dd/MM/YYYY HH.mm.ss Z", "ISO8601"]
```

# Linux 管道输出到文件时不做buffer

* 参考： https://unix.stackexchange.com/questions/25372/turn-off-buffering-in-pipe

场景：在大文件big_file里面搜索一个很少见的字符串xxx，可能早已有输出，但是却由于输出数据较少，buffer一直不满，迟迟不输出到a.txt。
例如：

```shell
grep xxx big_file > a.txt
```

用unbuffer命令即可让输出立即到文件中
```
unbuffer grep xxx big_file > a.txt
```
