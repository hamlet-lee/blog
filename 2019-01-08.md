# 尽量保证Kafka Producer数据不丢失

* 参考1： https://blog.csdn.net/john2522/article/details/64555065

* 参考2： https://blog.csdn.net/Lin_wj1995/article/details/80264599

```
如果对性能要求不高的话，可以再 producer.send() 方法调用后再调用 producer.flush() 方法，该方法会将数据全部生产到Kafka，否则就会阻塞。对于 producer.flush() 方法，源码原话如下：

"Flush any accumulated records form the producer. Blocks until all sends are complete."
```

我觉得，如果允许一定量丢失，但不允许大量丢失的话，有一个折衷办法：

每10000个send，flush一次

# Logstash 如果出错，可以把出错数据单独输出

参考： https://discuss.elastic.co/t/exception-handling/38989

```
filter {
  date {
    ...
  }
}

output {
  if "_dateparsefailure" in [tags] {
    file {
      path => ".../date_parse_errors.log"
    }
  }
}
```
