# 尽量保证Kafka Producer数据不丢失

* 参考1： https://blog.csdn.net/john2522/article/details/64555065

* 参考2： https://blog.csdn.net/Lin_wj1995/article/details/80264599

```
如果对性能要求不高的话，可以再 producer.send() 方法调用后再调用 producer.flush() 方法，该方法会将数据全部生产到Kafka，否则就会阻塞。对于 producer.flush() 方法，源码原话如下：

"Flush any accumulated records form the producer. Blocks until all sends are complete."
```

我觉得，如果允许一定量丢失，但不允许大量丢失的话，有一个折衷办法：

每10000个send，flush一次

# Logstash 如果出错，可以把出错数据单独输出

参考： https://discuss.elastic.co/t/exception-handling/38989

```
filter {
  date {
    ...
  }
}

output {
  if "_dateparsefailure" in [tags] {
    file {
      path => ".../date_parse_errors.log"
    }
  }
}
```

# Logstash 处理多种日期格式

参考：https://discuss.elastic.co/t/logstash-how-to-handle-exceptions/37843

```
match => ["DATE", "dd/MM/YYYY HH.mm.ss Z", "dd/MM/YYYY HH.mm.ss Z", "ISO8601"]
```

# Linux 管道输出到文件时不做buffer

* 参考： https://unix.stackexchange.com/questions/25372/turn-off-buffering-in-pipe

场景：在大文件big_file里面搜索一个很少见的字符串xxx，可能早已有输出，但是却由于输出数据较少，buffer一直不满，迟迟不输出到a.txt。
例如：

```shell
grep xxx big_file > a.txt
```

用unbuffer命令即可让输出立即到文件中
```
unbuffer grep xxx big_file > a.txt
```

# 设定Gradle的Home目录

```
export GRADLE_USER_HOME=/Users/mrhaki/dev/gradle
```

# Logstash 如果报错，注意把tags显示出来，看清楚是哪一步出错！

```
[2019-01-08T18:46:47,974][WARN ][logstash.codecs.plain    ] Received an event that has a different character encoding than you configured. {:text=>"119.147.183.23 - - [07/Jan/2019:00:00:02 +0800] \\\"GET /apps/log.html?xxxx=yyy&type=enter&q=Ê=ç± HTTP/1.1\\\" ...", :expected_charset=>"UTF-8"}
[2019-01-08T18:46:48,005][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9601}
{
       ...
      "message" => "...",
         "tags" => [
        [0] "_grokparsefailure"
    ],
           "ts" => 0
}

```

`_grokparsefailure` 表明是grok失败，不是kv或者别的失败！


# Logstash input codec 问题
对于accesslog，如果有二进制垃圾信息，会导致整行不能parse
可以尝试这样解决：

```
input {
  file {
    codec => plain {
      charset => "US-ASCII"
    }
}
```

虽然仍然会是一堆垃圾，但不会造成整行无法读入处理

# Logstash kv 插件可能生成数组

```
For example, consider a source like from=me from=me. [from] will map to an Array with two elements: ["me", "me"]. 
```

如果希望去重
```
filter {
  kv {
    allow_duplicate_values => false
  }
}
```

# Logstash 排除一些输入文件
```
file {
  path => "E:/domain/DEV/application/logs/.log"
  exclude => [ "CBA*.log", "BPL*.log" , "ARC*.log" ]
  type => "esbbwlog"
  codec => multiline {
    patterns_dir => "/patterns/devbw"
    pattern => "^%{BWTIME}"
    negate => true
    what => "previous"
    max_lines => "1000"
  }
}
```
