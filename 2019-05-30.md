# 磁盘使用增长的问题
1. 首先 使用 du-some.sh；
    ```shell
    #!/bin/sh
    du -b --exclude=.git --max-depth=3 . > du-1.txt
    sleep 1200
    du -b --exclude=.git --max-depth=3 . > du-2.txt
    sleep 1200
    du -b --exclude=.git --max-depth=3 . > du-3.txt
    ```
    
    例如
    ```shell
    # 第一天
    ~/du-some.sh
    
    # 第二天du之前把第一天的结果文件改下名，然后再du
    mv du-1.txt du-1.txt.20190527
    ~/du-some.sh
    ```
1. 然后 使用 du_diff.pl
    ```perl
    #!/usr/bin/perl
    use POSIX;
    if(scalar(@ARGV) != 2 ) {
      die "usage: <du result1> <du result2>";
    }
    my $in1 = $ARGV[0];
    my $in2 = $ARGV[1];

    my %data;
    open my $f1,"<$in1";
    while(<$f1>){
      $_ =~ /(\d+)\t(.*?)\n/;
      #print "[$1][".$2."]\n";
      $data{$2}=$1;
    }
    close($f1);

    my %data2;
    open my $f2, "<$in2";
    while(<$f2>){
      $_ =~ /(\d+)\t(.*?)\n/;
      #print "[$1][".$2."]\n";
      if(defined $data{$2} ) {
        $data2{$2}=$1;
      }
    }
    close($f2);

    my %data_diff;
    foreach $k (keys %data2) {
      my $s1 = $data{$k};
      my $s2 = $data2{$k};
      my $d = $s2 - $s1;
      # 只考虑变化大于1GB的
      if( $d > 1024 * 1024 * 1024 ) {
        $data_diff{$k} = $d;
      }
      #print "$k\t$s1\t$s2\n";
    }

    foreach $k (sort keys %data_diff) {
      print "$k\t".$data_diff{$k}."\t".ceil($data_diff{$k} / 1024 / 1024 / 1024) . "GB\n";
    }
    ```
例如：
```shell
$ ~/du_diff.pl du-1.txt.20190527  du-1.txt
./data	20124294386	19GB
./data/kafka-logs	20124294386	19GB
./data/kafka-logs/xxx	11251302413	11GB
./data/kafka-logs/yyy	1778673447	2GB
./zookeeper-x.x.x	17703515555	17GB
./zookeeper-x.x.x/zk_data	17621496195	17GB
./zookeeper-x.x.x/zk_data/version-2	17621496195	17GB
```
就能发现，增长比较多的是kafka 和 zookeeper 的目录。


# 删除旧日志的脚本示例
```shell
$ cd /path/to/some && find -iname 'log-server.log*' -mtime +1 -exec rm {} \;
$ find /path/to/some/  -mtime +1  -type f -exec rm -rf {} \;
$ cd /path/to/resin/logs/ && find . -mtime +2 -name "access*" -or -mtime +2 -name "impr*" -or -mtime +2 -name "failurl*" -or -mtime +2 -name "appsidewarn*"  -or -mtime +3 -name "log.ds*"  -or -mtime +3 -name "log.apps*" -or -mtime +3 -name "performance*"  -or -mtime +3 -name "gc*"  -or -mtime +10 -name "log*"|xargs rm -f {}
```

# YARN Scheduler 一个造成小任务被大任务影响的特性 reservation
例如，yarn的log里面记录到
```text
2019-05-30 17:15:42,520 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Skipping scheduling since node th075.corp.yodao.com:38627 is reserved by application appattempt_1552317955022_134919_000001
```
查阅了下，说是如果有大的Container在等待分配，小的任务就没法运行。

参考： https://community.hortonworks.com/content/supportkb/203200/errorskipping-scheduling-since-node-45454-is-reser.html

```text
Cause:
This is an expected behavior when there is already pending request of large container and the NodeManager cannot fit that request.

In such a case, YARN Capacity Scheduler will reserve memory on all NodeMangers until any one NodeManager is able to fit the larger container request. All other smaller container requests will be waiting until resources for larger container is fulfilled.
Solution:
To clear the reserved container request, have one NodeManager which has available capacity to fulfill the container request.

About:
This article created by Hortonworks Support (Article: 000007600) on 2018-07-10 16:50
OS: All
Type: Configuration, Executing_Jobs
Version: HDP
```
