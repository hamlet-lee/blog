# Retrying Hive Meta Store Client - 带重试的Client
RetryingMetaStoreClient 这个类，可以看看。

# Hadoop Kerberos 相关
* https://www.jianshu.com/p/2039fe8c62a1
* https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/what_is_kerberos.html
* https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/HttpAuthentication.html

> What's important is that tickets can be passed on: an authenticated principal can obtain a ticket to a service, and pass that on to another process in the distributed system.   
一个principle拿到的 ticket 是可以传递给其他Service使用的  
It(keytab file) can hold many principals, so one can be created for, say, hdfs, which contails all its principals, hdfs/node1@HCLUSTER, hdfs/node2@HCLUSTER, ...etc. Thus only one keytab per service is needed.  
一个keytab可以包含多个pricipal，因此运行于多台机器的service可以只用一个keytab就能放下  
 When a principal authenticates with the KDC, it doesn't get any special authentication secrets —it gets a ticket to the Ticket Granting Service. This ticket can then be used to get tickets to other services —and, like any other ticket, can be forwarded. Equally importantly, the ticket will expire —forcing the principal to re-authenticate via the command line or a keytab.  
与 TGS 联络用的也是 Ticket，因此，这个 Ticket 也是可以 forward 的。  
The KDC contains 'a secret' shared with the principal. There is no public/private key system here, just a shared secret.  
KDC 与 用户用的是对称加密？如果secret key改变了，keytabs也就失效了？？？  

实例
> The Hadoop Configuration class instance used to initialise the client is used to retrieve the value of "dfs.namenode.kerberos.principal" —so identifying the service to which the client must have a valid ticket to talk to.  
The Hadoop Kerberos code (this is in Java, not the OS), asks the Kerberos Ticket Granting Service, the TGS, for a ticket to talk to the Namenode's principal. It does this in a request authenticated with the TGT received during the kinit process.  
kinit后， hadoop fs -ls 的过程。  RPC 客户端会申请 dfs.namenode.kerberos.principal 的沟通 ticket，然后用于与 namenode 沟通。

Hadoop Delegation Tokens
> to avoid problems such as having to have the client ask the TGT for a ticket to talk to individual Datanodes when reading or writing a file across the HDFS filesystem, or even handle the problem with a tens of thousands of clients having to refresh their Namenode tickets every few hours.  
为避免大量的KDC通信，Hadoop 有个 Hadoop Delegation Tokens 机制。-- 听起来类似Hadoop一个服务认了，Hadoop其他服务也会认。

Block Token
> To get at these blocks, HDFS gives an authenticated caller a Block Tokens for every block they need to read in a file. The caller then requests the blocks of any of the datanodes hosting that block, including the block token in the request.  
客户端与DataNode沟通时，用Block Token.

Authentication Tokens
> Service自定义的给caller使用的token，如果失效了用户需要申请一个新的。这个过程可能会追溯到向 TGS 重新要 Ticket。甚至追溯到重新 Login从而拿到新的TGT。

Delegation Tokens
> 在Hadoop内可以直接renew的Token  
For the HDFS Client protocol, the client protocol itself is the token renewer. A client may talk to the Namenode using its current token, and request a new one, so refreshing it.  
HDFS Client 协议中直接就做了refresh 的工作。

提交YARN任务的例子：
> To give the YARN application the same rights to HDFS, the client-side application must request a Delegation Token to talk to HDFS, a key which is then passed to the YARN application in the ContainerLaunchContext within the ApplicationSubmissionContext used to define the application to launch: its required container resources, artifacts to download, "localize", environment to set up and command to run.  
YARN Client 用户先跟 HDFS 要到 Delegation Token，然后提交YARN任务时带上 Delegation Token。这样，YARN Jobs 运行时就可以用该用户的权限来运行。  
其实本质上就是： NameNode 自己发的Delegation Token，自己认即可。其他服务就是传来传去而已。

> This is exactly what YARN does: a token is issued by the YARN Resource Manager to an application instance's Application Manager at launch time; this is used in all communications from the AM to the RM.  
YARN 上 AM 与 RM 沟通，也有专门的token，token由 RM 发放。

运行Application要注意什么？
> If the client is required to run on a kerberos-authenticated account (e.g. kinit or keytab), then your main concern is simply making sure the principal is logged in.  
用户通过 kinit 或 keytab 来运行，只要保证login即可  

Proxy Users
> 拥有“伪装”权限的principal？
> You can configure proxy user using properties hadoop.proxyuser.$superuser.hosts along with either or both of hadoop.proxyuser.$superuser.groups and hadoop.proxyuser.$superuser.users.  
By specifying as below in core-site.xml, the superuser named super can connect only from host1 and host2 to impersonate a user belonging to group1 and group2.  
你可以配置某个user可以是proxyuser，并且可以配置它必须来自某个hosts，只能impersonate某个组的人。
