# 调研Presto Gateway
## 项目网址
https://github.com/lyft/presto-gateway  

## 运行起来，试试CLI方式能不能正常工作  
 直接mvn install 报错。删除掉pom.xml中checkstyle相关的plugin之后，能够正常build  
 配置文件 gateway/src/main/resources/config.yml.test  
  ```yaml
  requestRouter:
    port: my-proxy-port
    name: Proxly
    cacheDir: log/prestoproxy/cache
    historySize: 1000

  backends:
    - localPort: my-local-port1
      name: presto1
      proxyTo: http://my-host-1:my-port-1

    - localPort: my-local-port2
      name: presto2
      proxyTo: http://my-host-2:my-port-2

  server:
    applicationConnectors:
      - type: http
        port: 23145
    adminConnectors:
      - type: http
        port: 23146

  notifier:
    smtpHost: my-smtp-host
    smtpPort: my-smtp-port
    sender: my-sender@mydomain.com
    recipients:
      - myname@mydomain.com

  modules:
    - com.lyft.data.gateway.module.ProxyBackendProviderModule
    - com.lyft.data.gateway.module.GatewayProviderModule
    - com.lyft.data.gateway.module.NotifierModule

  managedApps:
    - com.lyft.data.gateway.GatewayManagedApp
    - com.lyft.data.gateway.ActiveClusterMonitor

  # Logging settings.
  logging:
    # The default level of all loggers. Can be OFF, ERROR, WARN, INFO, DEBUG, TRACE, or ALL.
    level: INFO

    # Logger-specific levels.
    loggers:
      com.lyft: DEBUG

    appenders:
      - type: console
      - type: file
        currentLogFilename: log/prestoproxy/prestoproxy-java.log
        archivedLogFilenamePattern: log/prestoproxy/prestoproxy-java-%d{yyyy-MM-dd}-%i.log.gz
        archivedFileCount: 7
        timeZone: UTC
        maxFileSize: 100MB

  ```
  
  ```shell
  cd gateway/target/
  java -jar gateway-1.4.4-jar-with-dependencies.jar server ../src/main/resources/config.yml.test
  ```
  CLI 连接 my-proxy-port 能够正常工作
  ```
  ./presto --server my-proxy-host:my-proxy-port --catalog my-default-category --schema my-default-schema
  ```
## 检查其它API
* 查询列表的API是否正常
查询在执行中，从 proxy http://my-proxy-host:my-proxy-port/v1/query 中查到的状态类似如下
```json
[..., {
    "queryId": "20190510_094037_00004_b6jbf",
    "session": {
      "queryId": "20190510_094037_00004_b6jbf",
      "transactionId": "18c87cef-8de3-41c9-a33f-be6aac4ca86c",
      "clientTransactionSupport": true,
      "user": "xxx",
      "source": "presto-cli",
      "catalog": "xxx",
      "schema": "xxx",
      "timeZoneKey": 2201,
      "locale": "en_US",
      "remoteUserAddress": "xxx",
      "userAgent": "StatementClient/0.185",
      "clientTags": [],
      "resourceEstimate": {},
      "startTime": 1557481237246,
      "systemProperties": {},
      "catalogProperties": {},
      "preparedStatements": {}
    },
    "state": "RUNNING",
    "memoryPool": "general",
    "scheduled": false,
    "self": "http://xxx/v1/query/20190510_094037_00004_b6jbf",
    "query": "select count(*) from xxx where day = '2019-05-04'",
    "queryStats": {
      "createTime": "2019-05-10T09:40:37.246Z",
      "elapsedTime": "18.48s",
      "executionTime": "18.37s",
      "totalDrivers": 117,
      "queuedDrivers": 27,
      "runningDrivers": 15,
      "completedDrivers": 58,
      "cumulativeUserMemory": 366025,
      "userMemoryReservation": "24B",
      "peakUserMemoryReservation": "24B",
      "totalCpuTime": "14.73s",
      "fullyBlocked": false,
      "blockedReasons": []
    }
  },
  ...
]
```
多次查询试验后，发现该查询的结果只是随机返回后端某个presto cluster 的结果而已，并没有做聚合 ...  
从proxy的日志上看，似乎它会从URL中的 queryId 判定该route到哪个presto cluster
```
DEBUG [2019-05-10 09:49:57,332] com.lyft.data.gateway.handler.QueryIdCachingProxyHandler: trying to extract query id from path [/v1/query] or queryString [null]
DEBUG [2019-05-10 09:49:57,332] com.lyft.data.gateway.handler.QueryIdCachingProxyHandler: query id in url [null]
INFO  [2019-05-10 09:49:57,332] com.lyft.data.gateway.handler.QueryIdCachingProxyHandler: Rerouting [x.x.x.x/v1/query]--> [http://my-host-1:my-port-1/v1/query]
DEBUG [2019-05-10 09:49:57,332] com.lyft.data.proxyserver.ProxyImpl: Target : http://my-host-1:my-port-1/v1/query
DEBUG [2019-05-10 09:49:57,333] com.lyft.data.proxyserver.ProxyImpl.Proxly: 2095037489 rewriting: http://my-proxy-host:my-proxy-port/v1/query -> http://my-host-1:my-port-1/v1/query
DEBUG [2019-05-10 09:49:57,333] com.lyft.data.proxyserver.ProxyImpl.Proxly: 2095037489 proxying to upstream:
GET /v1/query HTTP/1.1
Cookie: OUTFOX_SEARCH_USER_ID=-888291994@10.168.17.13; JSESSIONID=4CF45DAFEEE4BBBA09B64989D22AEC17
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Cache-Control: max-age=0
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0
Connection: keep-alive
Host: hd020:23142
Accept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2
Accept-Encoding: gzip, deflate
proxytarget: http://hd012:28286

HttpRequest[GET /v1/query HTTP/1.1]@78e02772

DEBUG [2019-05-10 09:49:57,359] com.lyft.data.proxyserver.ProxyImpl.Proxly: 2095037489 proxying to downstream:
HttpResponse[HTTP/1.1 200 OK]@174a9396
Date: Fri, 10 May 2019 09:49:57 GMT
Content-Type: application/json
X-Content-Type-Options: nosniff
Vary: Accept-Encoding, User-Agent
Content-Encoding: gzip
Transfer-Encoding: chunked

DEBUG [2019-05-10 09:49:57,359] com.lyft.data.proxyserver.ProxyImpl.Proxly: [2095037489] proxying content to downstream: [10] bytes
DEBUG [2019-05-10 09:49:57,359] com.lyft.data.gateway.handler.QueryIdCachingProxyHandler: SKIPPING For /v1/query
DEBUG [2019-05-10 09:49:57,360] com.lyft.data.proxyserver.ProxyImpl.Proxly: [2095037489] proxying content to downstream: [13024] bytes
DEBUG [2019-05-10 09:49:57,360] com.lyft.data.gateway.handler.QueryIdCachingProxyHandler: SKIPPING For /v1/query
DEBUG [2019-05-10 09:49:57,369] com.lyft.data.proxyserver.ProxyImpl.Proxly: [2095037489] proxying content to downstream: [5710] bytes
DEBUG [2019-05-10 09:49:57,370] com.lyft.data.gateway.handler.QueryIdCachingProxyHandler: SKIPPING For /v1/query
DEBUG [2019-05-10 09:49:57,370] com.lyft.data.proxyserver.ProxyImpl.Proxly: 2095037489 proxying successful
DEBUG [2019-05-10 09:49:57,370] com.lyft.data.proxyserver.ProxyImpl.Proxly: 2095037489 proxying complete
```
经查看，QueryIdCachingProxyHandler的确会从 url 或者 SQL 内容(当执行kill query语句时)中获取queryId。
 1. 如果之前有记录queryId -> backendURL，就会route过去；
 1. 如果之前没有记录，会选择一个backendURL放入Request Header。  
在返回时，获取 Response中的 id，并保存 id -> backendURL 的关系   

另外，从 HeaderBasedProxyHandler 可以看出，有个 "label" 方式的路由。
```java
String targetLabel = request.getHeader("label");
// ...
ProxyBackendConfiguration targetDestConfig = backendNameMap.get(targetLabel);
```
这个label其实就是上面的 name，即 “presto1” 和 “presto2”

* Kill 查询 的 API
按上面的分析，应该没问题

## 开发工具
* html template：用的是FreeMaker
* 前端用了
  * jquery
  * hbar-chart(好像没什么名气，大概就是画个柱状图)

## gateway重启后是否影响功能
不影响, 见 RoutingManager
```java
    PersistentCacheManager persistentCacheManager =
        CacheManagerBuilder.newCacheManagerBuilder()
            .with(CacheManagerBuilder.persistence(new File(cacheDataDir, "queryIdBackendMapping")))
            .withCache(
                "queryIdBackendPersistentCache",
                CacheConfigurationBuilder.newCacheConfigurationBuilder(
                    String.class,
                    String.class,
                    ResourcePoolsBuilder.newResourcePoolsBuilder()
                        .heap(1000, EntryUnit.ENTRIES)
                        .offheap(100, MemoryUnit.MB)
                        .disk(1, MemoryUnit.GB, true)))
            .build(true);

```
cache是有磁盘做后盾的。

## 检查jdbc方式连接是否正常
* 用yanagishima连接，没问题

## 检查长时间查询是否正常
* 用yanagishima发送的查询，295秒返回成功

## 配置多个presto集群，看是否有load balance效果
见 RoutingManager，用的是轮询策略
```java
  /**
   * Performs routing to an adhoc backend.
   *
   * @return
   */
  public String provideAdhocBackendForThisRequest() {
    List<ProxyBackendConfiguration> backends = this.gatewayBackendManager.getActiveAdhocBackends();
    int backendId = (int) (requestAdhocCounter.incrementAndGet() % backends.size());
    if (requestAdhocCounter.get() >= Long.MAX_VALUE - 1) {
      requestAdhocCounter.set(0);
    }
    return backends.get(backendId).getProxyTo();
  }
```
## 找到可以插入路由规则的地方

# 检查Kafka Topic的脚本
~/check.sh 
```shell
#!/bin/sh
export TOPIC=$1
echo 
echo ========== $TOPIC ==========
echo 
/path/to/kafka_2.11-0.10.0.0/bin/kafka-console-consumer.sh --zookeeper zk1:2185,zk2:2185,zk3:2185 --from-beginning --max-messages 10000 --topic $TOPIC | grep Processed
```
