# Kafka 数据压缩问题
参考： http://zhongmingmao.me/2019/08/02/kafka-compression/
```
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "all");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
// 开启GZIP压缩
// Producer启动后，生产的每个消息集合都会经过GZIP压缩，能够很好地节省网络传输带宽和Kafka Broker端的磁盘占用
props.put("compression.type", "gzip");

Producer<String, String> producer = new KafkaProducer<>(props);
```


Producer采用GZIP压缩算法，Broker采用Snappy压缩算法
Broker接收到GZIP压缩消息后，只能解压后使用Snappy压缩算法重新压缩一遍
Broker端也有compression.type参数，默认值是producer，表示Broker端会尊重Producer端使用的压缩算法
一旦Broker端设置了不同的compression.type，可能会发生预料之外的压缩/解压缩操作，导致CPU使用率飙升
