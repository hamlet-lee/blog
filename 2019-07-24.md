# HDFS DataNode 统计数据的解读
* Block Pool Used  
The used space by the block pool on all data nodes.
注意，是：   DFS用量 / 磁盘总容量。 这个值是10%，并不代表还有90%可用。因为可能剩余的90%的磁盘已经被其它人占用了！
* DFS Remaining  
The remaining capacity.  
DFS还能用的空间量，是否包括如下的预留量（简称reserved）？
```xml
        <property>
                <name>dfs.datanode.du.reserved</name>
                <value>214748364800</value>
                <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.</description>
        </property>
```
如上表示，每个磁盘预留 214 GB。如果与NodeManager混合部署，则这个数值应该要跟 NodeManager的配置相配合(简称healthRatio)。

```xml
        <property>
                <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
                <value>95.0</value>
                <description>The maximum percentage of disk space that may be utilized before a disk is marked as unhealthy by the disk checker service. This check is run for every disk used by the NodeManager. The default value is 90 i.e. 90% of the disk can be used.
<description>
        </property>
```

即 DFS预留的空间(reserved) > 磁盘大小 * (1-healthRatio) 
例如：我们的磁盘是 3.6T 的。

214 GB > 3.6T * (1-healthRatio) = 3.6T * 0.05 = 184 GB

基本成立。

不过，如果出现任务大量写本地日志的意外情况，就比较危险了，因果链条如下：
1. 本地日志量突增，会导致剩余空间小于5%
1. node manager会拒绝工作（根据healthRatio）
1. 此时，reserved < 实际剩余空间，HDFS 系统会暂时将写数据的任务调度到其它磁盘。
1. 如果HDFS的balancer在运行，则该磁盘上的数据会移动到其它节点，其实际剩余空间会增加，直至 reserved > 实际剩余空间

综上，有两个要点：
1. 配置要合理： DFS预留的空间(reserved) > 磁盘大小 * (1-healthRatio) 
1. balancer 要及时运行

# TCP连接关闭的“四次挥手”
参考：https://lanjingling.github.io/2016/02/27/nginx-tomcat-time-wait/

![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-07-24/tcp_close.jpg)  

按上面的链接的建议，可以考虑修改系统的/etc/sysctl.conf配置来减少TIME_WAIT的tcp连接：
```shell
vi /etc/sysctl.conf
net.ipv4.tcp_syncookies = 1  （某些情况下该参数已启用）
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
```
然后执行/sbin/sysctl -p让参数生效。再用命令查看TIME_WAIT连接数 
```shell
netstat -ae | grep "TIME_WAIT" |wc -l
```
发现大量的TIME_WAIT 已不存在。
```text
net.ipv4.tcp_syncookies = 1表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
net.ipv4.tcp_tw_reuse = 1表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
net.ipv4.tcp_tw_recycle = 1表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
net.ipv4.tcp_fin_timeout修改系統默认的TIMEOUT时间
```

https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux

However, as stated by tcp(7) manual page, the net.ipv4.tcp_tw_recycle option is quite problematic for public-facing servers as it won’t handle connections from two different computers behind the same NAT device, which is a problem hard to detect and waiting to bite you:

从 https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4396e46187ca5070219b81773c4e65088dac50cc
来看，
```text
The tcp_tw_recycle was already broken for connections
behind NAT, since the per-destination timestamp is not
monotonically increasing for multiple machines behind
a single destination address.

After the randomization of TCP timestamp offsets
in commit 8a5bd45f6616 (tcp: randomize tcp timestamp offsets
for each connection), the tcp_tw_recycle is broken for all
types of connections for the same reason: the timestamps
received from a single machine is not monotonically increasing,
anymore.

Remove tcp_tw_recycle, since it is not functional. Also, remove
the PAWSPassive SNMP counter since it is only used for
tcp_tw_recycle, and simplify tcp_v4_route_req and tcp_v6_route_req
since the strict argument is only set when tcp_tw_recycle is
enabled.
```
tcp_tw_recycle was already broken ...

# 用tcpdump查nginx至tomcat的问题
在nginx机器运行
```shell
tcpdump -i ${DEVICE_NAME} -A host ${TOMCAT_IP}  -n | grep -C100 $YOUR_URI
```
即可看到HTTP请求内容，查问题。

# 调研 Shiro
一个Java安全框架。

# 调研Presto的jdbc认证
如果jdbc中提供user和pass，则Presto客户端会拒绝连接。
```java
String connStr = "jdbc:presto://my-presto.mydomain.com:80/hive";
		SqlHelper sqlHelper = new SqlHelper("com.facebook.presto.jdbc.PrestoDriver",
				connStr);
		Object obj = sqlHelper.query("select 1", "myname", "abc");
		System.out.println(obj.toString());
```
报错：
```
Caused by: java.sql.SQLException: Authentication using username/password requires SSL to be enabled
```

参考：
https://prestodb.github.io/docs/current/installation/jdbc.html


客户端配置示例：
参考： https://support.treasuredata.com/hc/en-us/articles/360000708727-Presto-JDBC-Connection
```text
jdbc:presto://api-presto.treasuredata.com:443/td-presto?SSL=true
jdbc:presto://api-presto.treasuredata.com:443/td-presto/sample_datasets?SSL=true
```

参考： https://github.com/prestodb/presto/issues/11806  
如果需要一个私有的https，可以这么配置

```properties
coordinator=true
node-scheduler.include-coordinator=true
http-server.http.port=8080
query.max-memory=5GB
query.max-memory-per-node=1GB
discovery-server.enabled=true
discovery.uri=http://192.168.224.157:8080
node.internal-address=192.168.224.157
# 关键是下面几个属性
http-server.https.port=8889
http-server.https.enabled=true
# 也可以写成 etc/xxx.jks 的相对路径的形式
http-server.https.keystore.path=/opt/presto-server-0.206/keystore.jks
http-server.https.keystore.key=password
```

客户端这样写连接串
```text
String connStr = "jdbc:presto://host_in_inner_net:8889/hive?socksProxy=proxyhost:proxyport&SSL=true&SSLTrustStorePath=client.truststore.jks&SSLTrustStorePassword=xxx";
```
因为服务在机房内网，因此还配置了socks代理。

报错
```
Caused by: java.io.UncheckedIOException: javax.net.ssl.SSLPeerUnverifiedException: Hostname host_in_inner_net not verified:
...

然后参考 https://support.mulesoft.com/s/article/How-to-solve-the-problem-of-getting-Hostname-Verification-Exception-in-HTTP-Requester

需要在生成证书时提供dns名称，例如
```
keytool -genkey -alias server  -keyalg RSA -keystore <server key store file> -ext SAN="dns:foo.com,dns:blahblah.com,ip:127.0.0.1"
```
