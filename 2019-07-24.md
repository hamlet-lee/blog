# HDFS DataNode 统计数据的解读
* Block Pool Used  
The used space by the block pool on all data nodes.
注意，是：   DFS用量 / 磁盘总容量。 这个值是10%，并不代表还有90%可用。因为可能剩余的90%的磁盘已经被其它人占用了！
* DFS Remaining  
The remaining capacity.  
DFS还能用的空间量，是否包括如下的预留量（简称reserved）？
```xml
        <property>
                <name>dfs.datanode.du.reserved</name>
                <value>214748364800</value>
                <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.</description>
        </property>
```
如上表示，每个磁盘预留 214 GB。如果与NodeManager混合部署，则这个数值应该要跟 NodeManager的配置相配合(简称healthRatio)。

```xml
        <property>
                <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
                <value>95.0</value>
                <description>The maximum percentage of disk space that may be utilized before a disk is marked as unhealthy by the disk checker service. This check is run for every disk used by the NodeManager. The default value is 90 i.e. 90% of the disk can be used.
<description>
        </property>
```

即 DFS预留的空间(reserved) > 磁盘大小 * (1-healthRatio) 
例如：我们的磁盘是 3.6T 的。

214 GB > 3.6T * (1-healthRatio) = 3.6T * 0.05 = 184 GB

基本成立。

不过，如果出现任务大量写本地日志的意外情况，就比较危险了，因果链条如下：
1. 本地日志量突增，会导致剩余空间小于5%
1. node manager会拒绝工作（根据healthRatio）
1. 此时，reserved < 实际剩余空间，HDFS 系统会暂时将写数据的任务调度到其它磁盘。
1. 如果HDFS的balancer在运行，则该磁盘上的数据会移动到其它节点，其实际剩余空间会增加，直至 reserved > 实际剩余空间

综上，有两个要点：
1. 配置要合理： DFS预留的空间(reserved) > 磁盘大小 * (1-healthRatio) 
1. balancer 要及时运行

# TCP连接关闭的“四次挥手”
参考：https://lanjingling.github.io/2016/02/27/nginx-tomcat-time-wait/

![img](https://raw.githubusercontent.com/hamlet-lee/blog/master/2019-07-24/tcp_close.jpg)  
