# Presto 的 set session 不能通过JDBC Client进行调用
```sql
set session query_max_execution_time = '1d';
select 1; 
```
使用JDBC方式调用，报错：
```text
ERROR: java.sql.SQLFeatureNotSupportedException: SET/RESET SESSION is not supported via JDBC. Use the setSessionProperty() method on PrestoConnection.
```

即，只能通过 PrestoConnection 对象调用。
它的实际实现，看起来是在客户端保存了一个Map, 每次Query时，传到服务端。

```java
    /**
     * Adds a session property (experimental).
     */
    public void setSessionProperty(String name, String value)
    {
        requireNonNull(name, "name is null");
        requireNonNull(value, "value is null");
        checkArgument(!name.isEmpty(), "name is empty");

        CharsetEncoder charsetEncoder = US_ASCII.newEncoder();
        checkArgument(name.indexOf('=') < 0, "Session property name must not contain '=': %s", name);
        checkArgument(charsetEncoder.canEncode(name), "Session property name is not US_ASCII: %s", name);
        checkArgument(charsetEncoder.canEncode(value), "Session property value is not US_ASCII: %s", value);

        sessionProperties.put(name, value);
    }

```
```java

    StatementClient startQuery(String sql, Map<String, String> sessionPropertiesOverride)
    {
        String source = firstNonNull(clientInfo.get("ApplicationName"), "presto-jdbc");
        Optional<String> traceToken = Optional.ofNullable(clientInfo.get("TraceToken"));
        Iterable<String> clientTags = Splitter.on(',').trimResults().omitEmptyStrings()
                .split(nullToEmpty(clientInfo.get("ClientTags")));

        Map<String, String> allProperties = new HashMap<>(sessionProperties);
        allProperties.putAll(sessionPropertiesOverride);
```

补充：我印象jdbc connection初始化时可以传入properties。

# 调研：如何将一个Hive表的数据做筛选后，做并行处理（该处理的开销比较大，单CPU需要1 ms ~ 3 ms 才能处理一条）
* 方案1：直接写Presto UDF的方式，由于不容易控制CPU个数，不太适合
* 方案2：通过改变Presto的Plan的代码，使它增加一个全交换步骤，这需要对presto非常熟悉...
* 方案3：通过Presto将筛选后的数据写到临时表，然后专门用一个程序来处理
这就涉及到：
  1. 写出到临时表
  1. 通知处理
  1. 读入结果表
可以考虑在Presto中初始化出结果表，然后在外部程序直接写到结果表对应的HDFS目录的方式。
```sql
create table temp.tmp_test_input with (format = 'JSON') as select 1 as a, 2 as b;
create table temp.tmp_test_output (result varchar) with (format = 'JSON');
```
放置如下文件到 temp.tmp_test_output 的目录下，
result.json 
```json
{"result": "abcdef" }
{"result": "1234" }
```
注意，每行是<LF>结尾，不能有多余空行。如果多了一个空行，会被认为有3行数据，且第三行数据值=第二行数据值。
这样，在presto中就能读取了。

tmp_test_input 的文件默认是.gz格式。

读取.gz文件
```java
InputStream fileStream = new FileInputStream(filename);
InputStream gzipStream = new GZIPInputStream(fileStream);
Reader decoder = new InputStreamReader(gzipStream, encoding);
BufferedReader buffered = new BufferedReader(decoder);
```

tmp_test_input如何分bucket？
```sql
create table temp.tmp_test_input2 with (format = 'JSON', bucketed_by = ARRAY['a'], bucket_count = 50) as select 1 as a, 2 as b;
```

执行会稍慢，生成的文件在HDFS上
```text
20190618_050601_00020_wtdsz_bucket-00000.gz
...
20190618_050601_00020_wtdsz_bucket-00049.gz
```

Hive 生成gz的方法
```text
CREATE TABLE temp.tmp_json_tbl (
    col1 string
)
 
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
 
STORED AS TEXTFILE;

set hive.exec.compress.output=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
 
insert overwrite table temp.tmp_json_tbl select col1 from xxx where day = '2019-12-12';
```

Hive CTAS 语法生成 gz
```text
set hive.exec.compress.output=true;set 
mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;

create table temp. temp.tmp_json_tbl2
  ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
  STORED as TEXTFILE
  as
    select col1 from xxx where day = '2019-12-12';
```

# 网络传输速度测试
参考： https://blog.csdn.net/yxwmzouzou/article/details/78839356
```
yum -y install epel* 
yum -y linstall iperf3*

服务端iperf3 -s 
客户端 
iperf3 -c 10.96.45.162 -u -i 1 -t X 
iperf3 -c 10.96.45.162 -w 4k -i 1 -t 60 #4K 
iperf3 -c 10.96.45.162 -w 4k -i 1 -t 60 -P 10 
-u：使用UDP，而非TCP
-c：客户端模式，后接服务器ip 
-p：后接服务端监听的端口 
-i：设置带宽报告的时间间隔，单位为秒 
-t：设置测试的时长，单位为秒 
-w：设置tcp窗口大小，一般可以不用设置，默认即可
```

# 建立 NPM 项目，用于测试执行js的效率
* 初始化项目
```shell
npm init
```

* 按行读取数据
参考https://stackabuse.com/reading-a-file-line-by-line-in-node-js/
```text
read lines=5000
g: duration: 1805 ms, 0.361ms each
h: duration: 1407 ms, 0.2814ms each

g: duration: 2081 ms, 0.4162ms each
h: duration: 1643 ms, 0.3286ms each

g: duration: 2511 ms, 0.5022ms each
h: duration: 1600 ms, 0.32ms each

g: duration: 1753 ms, 0.3506ms each
h: duration: 1310 ms, 0.262ms each

g: duration: 1836 ms, 0.3672ms each
h: duration: 1461 ms, 0.2922ms each

如果2亿条数据，在32 * 40个进程的情况下运行。需要多少秒？
2 * 100000000 / (32 * 40) / (1000 / 0.3672)
= 57秒
```

# Node.js 多线程
https://blog.logrocket.com/node-js-multithreading-what-are-worker-threads-and-why-do-they-matter-48ab102f8b10/

# Node.js 读写HDFS?
* https://dzone.com/articles/accessing-bigdata-hadoop-hdfs-data-by-using-nodejs
* https://hub.packtpub.com/introduction-using-nodejs-hadoops-mapreduce-jobs/
* 有关ha模式的讨论： https://community.hortonworks.com/questions/61539/how-do-i-reference-a-high-availability-cluster-in.html
    * https://hadoop.apache.org/docs/r2.7.1/hadoop-hdfs-httpfs/ServerSetup.html
    * https://community.hortonworks.com/content/supportkb/49668/knox-using-webhdfs-with-namenode-ha.html
# Node.js gz文件压缩和解压
https://medium.com/@harrietty/zipping-and-unzipping-files-with-nodejs-375d2750c5e4

补充nodejs写出.gz的例子
参考：https://github.com/nodejs/node-v0.x-archive/issues/14479

```javascript
var fs = require('fs');
var output = fs.createWriteStream('test.gz');
var zlib = require('zlib');
var compress = zlib.createGzip();

compress.pipe(output);

compress.write('Hello, World!');
compress.end();
```

查看内容
```shell
$ gzip -d < test.gz 
Hello, World!
```

# Node.js 内存
http://lokijs.org/#/

# Node.js 读写 parquet
https://github.com/ironSource/parquetjs

# Node.js 直接处理HDFS压缩文本文件
* HDFS文件 -> 读流
https://www.npmjs.com/package/webhdfs
* 压缩流 -> 文本流
https://medium.com/@harrietty/zipping-and-unzipping-files-with-nodejs-375d2750c5e4
* 文本流 -> 逐行读取
https://github.com/jahewson/node-byline
